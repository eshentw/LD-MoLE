name: llama3.1_8b
model_name: "meta-llama/Llama-3.1-8B"
use_hf_weights: True

# Model-specific configurations
pad_token_id: 128001  # Llama 3 specific pad token ID

# Debugging configurations
output_attentions: True

# Lora configurations
apply_lora: True
lora:
  lora_freeze: False # lora head
  router_freeze: False # router
  sparsegen_init: "kaiming" # Options: "kaiming", "dense" (λ≈0, many adapters), "sparse" (λ≈1, few adapters), "zeros"
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  lora_nums: 8
  bias: "none"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  modules_to_save: # If you want to train and save classifier or other modules
    - "score"
    - "sparsegen"

  sparsegen_cfg:
    enabled: True
    input_sizes: [4096, 14336]
    hidden_sizes: 512
