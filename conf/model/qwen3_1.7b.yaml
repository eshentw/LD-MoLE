name: qwen3_1.7b
model_name: "Qwen/Qwen3-1.7B"
use_hf_weights: True

# Model-specific configurations
pad_token_id: 151643  # Qwen3 specific pad token ID

# Debugging configurations
output_attentions: True

# Lora configurations
apply_lora: True
lora:
  lora_freeze: False # lora head
  router_freeze: False # router
  sparsegen_init: "kaiming" # Options: "kaiming", "dense" (λ≈0, many adapters), "sparse" (λ≈1, few adapters), "zeros"
  use_abs: False # allow sparsegen z to be negative
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  lora_nums: 8
  bias: "none"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  modules_to_save: # If you want to train and save classifier or other modules
    - "score"
    # - "sparsegen"
  selected_topk: 2
  
  ReLUMoE_cfg:
    enabled: False
    normalize: False

  TopP_cfg:
    enabled: False
    p: 0.4
    
  sparsegen_cfg:
    enabled: True
    init_strategy: "kaiming" # Options: "kaiming", "dense" (λ≈0, many adapters), "sparse" (λ≈1, few adapters), "zeros"
    input_sizes: [2048, 6144] # other: 2048, down: 6144
    hidden_sizes: 256

# Add the following lines to specify the each "muti" pretrained LoRA paths
pretrained_multi_lora_paths: ???
cpkt_path: ???

# Quantization configurations
apply_quantization: False # Not working yet so stay False for now
quantization:
  quantization_config:
    load_in_4bit: True
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: True